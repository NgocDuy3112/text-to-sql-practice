import streamlit as st

import uuid
from shared.sql_executor import ChatHistory, execute_sql_to_df
from shared.messages import ChatbotMessage, MESSAGE_TYPE
from shared.chat_model import llm_chat_base
import os
import yaml

from implements.P03_router import route_question, ROUTER_QUESTION
from implements.P04_preprocessor import preprocess_question
from implements.P05_context_retriever import retrieve_context


# =========================
# Load prompts t·ª´ file YAML
# =========================
prompt_file = os.path.join(os.path.dirname(__file__), "implements", "P06_prompt.yml")
with open(prompt_file, 'r', encoding='utf-8') as f:
    PROMPTS = yaml.safe_load(f)


# =========================
# Page config
# =========================
st.set_page_config(
    page_title="LLM SQL Chatbot Demo",
    layout="wide"
)


# =========================
# DB config
# =========================
DB_PATH = os.path.join(os.path.dirname(__file__), "db", "demo.db")

# =========================
# Session state init
# =========================
if "active_conversation_id" not in st.session_state:
    st.session_state.active_conversation_id = None
if "last_processed_input" not in st.session_state:
    st.session_state.last_processed_input = None


# =========================
# H√†m h·ªó tr·ª£ qu·∫£n l√Ω conversation
# =========================
def create_new_conversation():
    """T·∫°o conversation m·ªõi v·ªõi title t·ª± ƒë·ªông tƒÉng"""
    # N·∫øu ƒë√£ c√≥ conversation active v√† conversation ƒë√≥ ch∆∞a c√≥ message user th√¨ kh√¥ng t·∫°o m·ªõi
    cid = st.session_state.active_conversation_id
    if cid:
        chat = ChatHistory(DB_PATH, cid)
        messages = chat.get()
        # N·∫øu ch·ªâ c√≥ message h·ªá th·ªëng (ho·∫∑c r·ªóng), kh√¥ng t·∫°o m·ªõi
        user_msgs = [m for m in messages if m.type == MESSAGE_TYPE.USER]
        if not user_msgs:
            # ƒê√£ c√≥ conversation r·ªóng, ch·ªâ chuy·ªÉn sang n√≥
            return
    
    # T·∫°o m·ªõi conversation
    conv_id = str(uuid.uuid4())
    
    # T√≠nh s·ªë th·ª© t·ª± conversations ƒë√£ t·∫°o (bao g·ªìm c·∫£ ƒë√£ x√≥a)
    # L·∫•y max s·ªë t·ª´ c√°c title hi·ªán c√≥
    all_convs = ChatHistory.list_conversations(DB_PATH)
    max_num = 0
    for existing_cid in all_convs:
        existing_title = ChatHistory.get_conversation_title(DB_PATH, existing_cid)
        if existing_title and existing_title.startswith("Conversation "):
            try:
                num = int(existing_title.replace("Conversation ", ""))
                max_num = max(max_num, num)
            except ValueError:
                pass
    
    title = f"Conversation {max_num + 1}"
    
    # T·∫°o conversation trong database
    ChatHistory.create_conversation(DB_PATH, conv_id, title)
    st.session_state.active_conversation_id = conv_id


def delete_conversation(conv_id):
    ChatHistory.delete_conversation(DB_PATH, conv_id)
    if st.session_state.active_conversation_id == conv_id:
        # Ch·ªçn conversation kh√°c n·∫øu c√≥, n·∫øu kh√¥ng th√¨ None
        all_convs = ChatHistory.list_conversations(DB_PATH)
        st.session_state.active_conversation_id = all_convs[0] if all_convs else None


def get_active_conversation():
    cid = st.session_state.active_conversation_id
    if cid:
        # Ki·ªÉm tra xem conversation c√≥ t·ªìn t·∫°i kh√¥ng
        all_convs = ChatHistory.list_conversations(DB_PATH)
        if cid in all_convs:
            return ChatHistory(DB_PATH, cid)
        else:
            # Conversation ƒë√£ b·ªã x√≥a, reset v·ªÅ None
            st.session_state.active_conversation_id = None
    return None


# =========================
# H√†m h·ªó tr·ª£ chatbot
# =========================
def format_table_context(tables: list) -> str:
    """Format danh s√°ch TableDescription th√†nh chu·ªói ng·ªØ c·∫£nh"""
    context_parts = []
    for table in tables:
        table_info = f"> B·∫£ng `{table.name}`\n"
        table_info += f"  - M√¥ t·∫£ b·∫£ng: {table.description}\n"
        table_info += f"  - Danh s√°ch c·ªôt:\n"
        
        columns_info = []
        for col in table.table_columns:
            columns_info.append(f"    + C·ªôt `{col.column_name}`: {col.column_description}")
        table_info += "\n".join(columns_info)
        
        context_parts.append(table_info)
    
    return "\n\n".join(context_parts)


def stream_llm_response(prompt: str):
    """Generator ƒë·ªÉ streaming response t·ª´ LLM"""
    msg = llm_chat_base._to_langchain_prompt(prompt)
    
    for chunk in llm_chat_base.model.stream(msg):
        if hasattr(chunk, 'content') and chunk.content:
            yield chunk.content

# =========================
# ƒê·ªãnh nghƒ©a lu·ªìng chatbot
# =========================
def run_llm_pipeline(user_message: str, conversation: ChatHistory, status_placeholder):
    """
    Pipeline x·ª≠ l√Ω chatbot:
    1. Preprocess question
    2. Route question
    3. N·∫øu NON_QUERY: tr·∫£ l·ªùi tr·ª±c ti·∫øp (streaming)
    4. N·∫øu QUERY: retrieve context -> generate SQL -> execute -> generate answer (streaming)
    
    Returns: tuple (generator, debug_info)
    """
    debug_info = {
        "processed_question": "",
        "route": "",
        "relevant_table": "",
        "sql_query": "",
        "sql_result": ""
    }
    
    # L·∫•y l·ªãch s·ª≠ chat ƒë·ªÉ preprocess
    chat_history = conversation.get()[:-1]
    
    # ===== B∆∞·ªõc 1: Chu·∫©n h√≥a c√¢u h·ªèi =====
    
    with status_placeholder.spinner("üîç ƒêang chu·∫©n h√≥a c√¢u h·ªèi..."):
        processed_question = preprocess_question(user_message, chat_history)
    
    debug_info["processed_question"] = processed_question
    
    # ===== H·∫øt B∆∞·ªõc 1 =====
    
    # ===== B∆∞·ªõc 2: Ph√¢n ƒë·ªãnh lu·ªìng =====
    
    with status_placeholder.spinner("üö¶ ƒêang ph√¢n ƒë·ªãnh lu·ªìng x·ª≠ l√Ω..."):
        route = route_question(processed_question)
    
    debug_info["route"] = route.value
    
    # ===== H·∫øt B∆∞·ªõc 2 =====
    
    if route == ROUTER_QUESTION.NON_QUERY:
        # ===== Lu·ªìng NON_QUERY =====
        
        status_placeholder.empty()
        
        prompt = f"""
            B·∫°n l√† tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ ng∆∞·ªùi d√πng.

            ### C√¢u h·ªèi ng∆∞·ªùi d√πng:

            {processed_question}

            ### Y√™u c·∫ßu:

            H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch th√¢n thi·ªán v√† h·ªØu √≠ch."""
        
        return stream_llm_response(prompt), debug_info
    
        # ===== K·∫øt th√∫c ph√¢n lu·ªìng NON_QUERY =====
    else:
        # ===== Lu·ªìng QUERY =====
        
        # ===== B∆∞·ªõc 3: Truy v·∫•n t∆∞∆°ng ƒë·ªìng =====
        with status_placeholder.spinner("üìä ƒêang truy xu·∫•t ng·ªØ c·∫£nh c∆° s·ªü d·ªØ li·ªáu..."):
            tables = retrieve_context(processed_question, k=4)
            context = format_table_context(tables)
        
        debug_info["relevant_table"] = [t.name for t in tables]
        
        # ===== H·∫øt B∆∞·ªõc 3 =====
        
        # ===== B∆∞·ªõc 4: T·∫°o SQL =====
        with status_placeholder.spinner("üî® ƒêang t·∫°o truy v·∫•n SQL..."):
            sql_prompt = PROMPTS['generate_sql']
            prompt = f"""
                {sql_prompt}

                ### Schema b·∫£ng/c·ªôt truy v·∫•n:

                {context}

                ### C√¢u h·ªèi ng∆∞·ªùi d√πng:

                {processed_question}

                ### Y√™u c·∫ßu ƒë·∫ßu ra:

                Ch·ªâ tr·∫£ v·ªÅ c√¢u truy v·∫•n SQL, kh√¥ng k√®m theo gi·∫£i th√≠ch."""
            sql_query = llm_chat_base.generate(prompt).strip()
            
            # Lo·∫°i b·ªè markdown code block n·∫øu c√≥
            if sql_query.startswith("```"):
                lines = sql_query.split('\n')
                sql_query = '\n'.join(lines[1:-1]) if len(lines) > 2 else sql_query
            sql_query = sql_query.replace("```sql", "").replace("```", "").strip()
            
            debug_info["sql_query"] = sql_query
        
        # ===== H·∫øt B∆∞·ªõc 4 =====
        
        # ===== B∆∞·ªõc 5: Th·ª±c thi SQL =====
        with status_placeholder.spinner("‚ö° ƒêang th·ª±c thi truy v·∫•n..."):
            try:
                df = execute_sql_to_df(sql_query, DB_PATH)
                
                if not df.empty:
                    result_str = df.to_json(
                        orient="records", force_ascii=False,
                        lines=True, indent=2
                    )
                else:
                    result_str = "K·∫øt qu·∫£ tr·∫£ v·ªÅ r·ªóng."
                
                debug_info["sql_result"] = result_str
            except Exception as e:
                status_placeholder.error(f"‚ùå L·ªói th·ª±c thi SQL: {str(e)}")
                
                debug_info["sql_result"] = f"Error: {str(e)}"
                
                return iter([f"‚ùå L·ªói khi th·ª±c thi SQL: {str(e)}\n\nSQL: {sql_query}"]), debug_info
        
        # ===== H·∫øt B∆∞·ªõc 5 =====
        
        # ===== B∆∞·ªõc 6: T·∫°o c√¢u tr·∫£ l·ªùi (streaming) =====
        
        status_placeholder.empty()
        answer_prompt = PROMPTS['generate_answer']
        prompt = f"""
            {answer_prompt}

            ### C√¢u h·ªèi ng∆∞·ªùi d√πng:

            {processed_question}

            ### Truy v·∫•n SQL ƒë√£ th·ª±c thi:

            {sql_query}

            ### K·∫øt qu·∫£ truy v·∫•n:

            {result_str}

            ### Y√™u c·∫ßu ƒë·∫ßu ra

            H√£y t·∫°o c√¢u tr·∫£ l·ªùi t·ª± nhi√™n d·ª±a tr√™n k·∫øt qu·∫£ tr√™n."""
        
        return stream_llm_response(prompt), debug_info
        
        # ===== K·∫øt th√∫c ph√¢n lu·ªìng QUERY =====


# =========================
# UI: Sidebar
# =========================
with st.sidebar:
    st.title("üí¨ Chatbot Demo")

    if st.button("‚ûï Cu·ªôc tr√≤ chuy·ªán m·ªõi"):
        create_new_conversation()

    st.divider()

    for cid in ChatHistory.list_conversations(DB_PATH):
        # L·∫•y ti√™u ƒë·ªÅ t·ª´ b·∫£ng conversations
        title = ChatHistory.get_conversation_title(DB_PATH, cid)
        if not title:
            title = f"Conversation {cid[:8]}"
        
        # Ki·ªÉm tra xem ƒë√¢y c√≥ ph·∫£i conversation ƒëang active kh√¥ng
        is_active = (cid == st.session_state.active_conversation_id)
        
        col1, col2 = st.columns([4, 1])
        with col1:
            # S·ª≠ d·ª•ng type kh√°c nhau ƒë·ªÉ highlight active conversation
            button_type = "primary" if is_active else "secondary"
            if st.button(title, key=f"select_{cid}", type=button_type, use_container_width=True):
                st.session_state.active_conversation_id = cid
                st.rerun()
        with col2:
            if st.button("üóëÔ∏è", key=f"delete_{cid}"):
                delete_conversation(cid)
                st.rerun()


# =========================
# UI: Main chat area
# =========================
st.title("üìä Chatbot truy v·∫•n d·ªØ li·ªáu (Demo)")


chat = get_active_conversation()

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat n·∫øu c√≥ conversation active
if chat is not None:
    for msg in chat.get():
        role = "user" if msg.type == MESSAGE_TYPE.USER else ("assistant" if msg.type == MESSAGE_TYPE.ASSISTANT else "system")
        if role == "system":
            continue
        with st.chat_message(role):
            st.markdown(msg.content)
            # Hi·ªÉn th·ªã debug info cho assistant messages
            if role == "assistant" and msg.debug:
                with st.expander("üîç Th√¥ng tin debug", expanded=False):
                    if msg.debug.get("processed_question"):
                        st.write("**C√¢u h·ªèi chu·∫©n h√≥a:**")
                        st.code(msg.debug["processed_question"], language="text")
                    
                    if msg.debug.get("route"):
                        st.write("**Lu·ªìng x·ª≠ l√Ω:**")
                        st.code(msg.debug["route"], language="text")
                    
                    if msg.debug.get("relevant_table"):
                        st.write("**B·∫£ng li√™n quan:**")
                        st.code(msg.debug["relevant_table"], language="sql")
                    
                    if msg.debug.get("sql_query"):
                        st.write("**Truy v·∫•n SQL:**")
                        st.code(msg.debug["sql_query"], language="sql")
                    
                    if msg.debug.get("sql_result"):
                        st.write("**K·∫øt qu·∫£ truy v·∫•n:**")
                        st.code(msg.debug["sql_result"], language="text")
else:
    st.info("üí¨ Nh·∫•n n√∫t '‚ûï Cu·ªôc tr√≤ chuy·ªán m·ªõi' ·ªü sidebar ƒë·ªÉ b·∫Øt ƒë·∫ßu")


# =========================
# UI: Chat input
# =========================
if chat is not None:
    user_input = st.chat_input("Nh·∫≠p c√¢u h·ªèi...")

    if user_input and user_input != st.session_state.last_processed_input:
        # ƒê√°nh d·∫•u input ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·ªÉ tr√°nh duplicate
        st.session_state.last_processed_input = user_input
        
        # L∆∞u message user v√†o db
        chat.add(ChatbotMessage(type=MESSAGE_TYPE.USER, content=user_input))

        # Hi·ªÉn th·ªã message user
        with st.chat_message("user"):
            st.markdown(user_input)

        # G·ªçi pipeline LLM v·ªõi streaming
        with st.chat_message("assistant"):
            # T·∫°o placeholder cho status v√† message
            status_placeholder = st.empty()
            message_placeholder = st.empty()
            full_response = ""
            
            # Ch·∫°y pipeline v·ªõi status placeholder
            response_generator, debug_info = run_llm_pipeline(
                user_message=user_input,
                conversation=chat,
                status_placeholder=status_placeholder
            )
            
            # Stream response
            for chunk in response_generator:
                full_response += chunk
                message_placeholder.markdown(full_response + "‚ñå")
            
            # Hi·ªÉn th·ªã response ho√†n ch·ªânh
            message_placeholder.markdown(full_response)
            
            # Hi·ªÉn th·ªã debug info
            if debug_info:
                with st.expander("üîç Th√¥ng tin debug", expanded=False):
                    if debug_info.get("processed_question"):
                        st.write("**C√¢u h·ªèi chu·∫©n h√≥a:**")
                        st.code(debug_info["processed_question"], language="text")
                    
                    if debug_info.get("route"):
                        st.write("**Lu·ªìng x·ª≠ l√Ω:**")
                        st.code(debug_info["route"], language="text")
                    
                    if debug_info.get("sql_query"):
                        st.write("**Truy v·∫•n SQL:**")
                        st.code(debug_info["sql_query"], language="sql")
                    
                    if debug_info.get("sql_result"):
                        st.write("**K·∫øt qu·∫£ truy v·∫•n:**")
                        st.code(debug_info["sql_result"], language="text")

        # L∆∞u message assistant v√†o db v·ªõi debug info
        chat.add(ChatbotMessage(type=MESSAGE_TYPE.ASSISTANT, content=full_response, debug=debug_info))
        
        # Reset state
        st.session_state.last_processed_input = None
        st.rerun()